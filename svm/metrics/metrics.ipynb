{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики качества классификации\n",
    "\n",
    "Данное задание основано на материалах лекций по метрикам качества классификации.\n",
    "\n",
    "## Вы научитесь:\n",
    "• вычислять различные меры качества классификации: долю правильных ответов, точность, полноту, AUC-ROC и т.д.\n",
    "\n",
    "• сравнивать алгоритмы классификации при наличии ограничений на точность или полноту\n",
    "\n",
    "## Введение\n",
    "\n",
    "В задачах классификации может быть много особенностей, влияющих на подсчет качества: различные цены ошибок, несбалансированность классов и т.д. Из-за этого существует большое количество метрик качества — каждая из них рассчитана на определенное сочетание свойств задачи и требований к ее решению.\n",
    "\n",
    "Меры качества классификации можно разбить на две большие группы: предназначенные для алгоритмов, выдающих номера классов, и для алгоритмов, выдающих оценки принадлежности к классам. К первой группе относятся доля правильных ответов, точность, полнота, F-мера. Ко второй — площади под ROC- или PR-кривой.\n",
    "\n",
    "## Реализация в Sklearn\n",
    "\n",
    "Загрузим файл classification.csv.В нем записаны истинные классы объектов выборки (колонка true) и ответы некоторого классификатора (колонка predicted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"classification.csv\")\n",
    "eq = df.query('true == pred') #TP и TN\n",
    "nq = df.query('true != pred') #FP и FN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним таблицу ошибок классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Positive</th>\n",
       "      <th>Actual Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted Positive</th>\n",
       "      <td>43</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Negative</th>\n",
       "      <td>34</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Actual Positive  Actual Negative\n",
       "Predicted Positive               43               59\n",
       "Predicted Negative               34               64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = eq.query('true == 1')\n",
    "TN = eq.query('true == 0')\n",
    "FP = nq.query('true == 0')\n",
    "FN = nq.query('true == 1')\n",
    "d = [\n",
    "    [TP.count()[0], FN.count()[0]],\n",
    "    [FP.count()[0], TN.count()[0]]\n",
    "]\n",
    "pd.DataFrame(data=d,columns=['Actual Positive','Actual Negative'],index=['Predicted Positive', 'Predicted Negative'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя Sklearn, посчитаем основные метрики качества классификатора:\n",
    "\n",
    "• Accuracy (доля верно угаданных) — sklearn.metrics.accuracy_score\n",
    "\n",
    "• Precision (точность) — sklearn.metrics.precision_score\n",
    "\n",
    "• Recall (полнота) — sklearn.metrics.recall_score\n",
    "\n",
    "• F-мера — sklearn.metrics.f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.535\n",
      "Precision: 0.5584415584415584\n",
      "Recall: 0.4215686274509804\n",
      "F-мера: 0.48044692737430167\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_true=df['true'],y_pred=df['pred'])\n",
    "precision = metrics.precision_score(y_true=df['true'],y_pred=df['pred'])\n",
    "recall = metrics.recall_score(y_true=df['true'],y_pred=df['pred'])\n",
    "f1 = metrics.f1_score(y_true=df['true'],y_pred=df['pred'])\n",
    "\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nF-мера: {}\".format(accuracy,precision,recall,f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется четыре обученных классификатора. В файле scores.csv записаны истинные классы и значения степени принадлежности положительному классу для каждого классификатора на некоторой выборке:\n",
    "\n",
    "• для логистической регрессии — вероятность положительного класса (колонка score_logreg),\n",
    "\n",
    "• для SVM — отступ от разделяющей поверхности (колонка score_svm),\n",
    "\n",
    "• для метрического алгоритма — взвешенная сумма классов соседей (колонка score_knn),\n",
    "\n",
    "• для решающего дерева — доля положительных объектов в листе (колонка score_tree).\n",
    "\n",
    "Загрузим этот файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.read_csv('scores.csv')\n",
    "target = df_score['true']\n",
    "data_logreg = df_score['score_logreg']\n",
    "data_svm = df_score['score_svm']\n",
    "data_knn = df_score['score_knn']\n",
    "data_tree = df_score['score_tree']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем площадь под ROC-кривой для каждого классификатора и найдём среди них наибольшее значение метрики AUC-ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logreg</th>\n",
       "      <td>0.719188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        roc_auc_score\n",
       "logreg       0.719188"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc = {}\n",
    "roc['logreg'] = metrics.roc_auc_score(y_true=target, y_score=data_logreg)\n",
    "roc['svm'] = metrics.roc_auc_score(y_true=target, y_score=data_svm)\n",
    "roc['knn'] = metrics.roc_auc_score(y_true=target, y_score=data_knn)\n",
    "roc['tree'] = metrics.roc_auc_score(y_true=target, y_score=data_tree)\n",
    "\n",
    "df_roc = pd.DataFrame(data=roc,index=['roc_auc_score']).transpose()\n",
    "df_roc.sort_values(['roc_auc_score'],ascending=False,inplace=True)\n",
    "df_roc.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем, какой классификатор достигает наибольшей точности (Precision) при полноте (Recall) не менее 70%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>0.651786</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      precision  recall\n",
       "tree   0.651786     1.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def calculate_precision(y_true=None, y_score=None):\n",
    "    prec, recall, threshold = precision_recall_curve(y_true, y_score)\n",
    "    dt = pd.DataFrame(prec,columns=['precision'])\n",
    "    dt['recall'] = recall\n",
    "    return dt.query('recall > 0.7').max()\n",
    "acc = {}\n",
    "acc['logreg']=calculate_precision(y_true=target, y_score=data_logreg)\n",
    "acc['svm']=calculate_precision(y_true=target, y_score=data_svm)\n",
    "acc['knn']=calculate_precision(y_true=target, y_score=data_knn)\n",
    "acc['tree']=calculate_precision(y_true=target, y_score=data_tree)\n",
    "\n",
    "df_prec = pd.DataFrame(data=acc).transpose()\n",
    "df_prec.sort_values(['precision'],ascending=False,inplace=True)\n",
    "df_prec.head(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
